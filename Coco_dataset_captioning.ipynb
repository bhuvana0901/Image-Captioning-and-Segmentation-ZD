{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOetU8vvpSO6DTqqTnbgZkN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvana0901/Image-Captioning-and-Segmentation-ZD/blob/main/Coco_dataset_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M5TpYL9AM-P2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "with open('dataset_coco.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "train_images = []\n",
        "val_images = []\n",
        "test_images = []\n",
        "\n",
        "for img in data['images']:\n",
        "    if img['split'] == 'train':\n",
        "        train_images.append(img)\n",
        "    elif img['split'] == 'val':\n",
        "        val_images.append(img)\n",
        "    elif img['split'] == 'test':\n",
        "        test_images.append(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_caption_pairs(images):\n",
        "    image_caption_pairs = []\n",
        "    for img in images:\n",
        "        filename = img['filename']\n",
        "        for sentence in img['sentences']:\n",
        "            caption = ' '.join(sentence['tokens'])\n",
        "            image_caption_pairs.append((filename, caption))\n",
        "    return image_caption_pairs\n",
        "\n",
        "train_pairs = get_image_caption_pairs(train_images)\n"
      ],
      "metadata": {
        "id": "nktQoKTxQGai"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "captions = [cap for _, cap in train_pairs]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(captions)\n",
        "\n",
        "# Save vocab size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert to sequences\n",
        "cap_seqs = tokenizer.texts_to_sequences(captions)\n",
        "cap_seqs = pad_sequences(cap_seqs, padding='post')\n"
      ],
      "metadata": {
        "id": "TU21ymrvSII5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_img(img_path):\n",
        "    img = image.load_img(img_path, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    return preprocess_input(x)\n"
      ],
      "metadata": {
        "id": "WweZkd6lSitN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko3onilPVa_7",
        "outputId": "76c42244-3d5f-4cba-defd-181b0c47c90a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "jCiTTSp2V_DN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Ep7xPt6OWF3_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.fc2(x)\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))\n"
      ],
      "metadata": {
        "id": "dfVoi8zLWKsH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "\n",
        "# Create encoder-decoder\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n"
      ],
      "metadata": {
        "id": "uOXTBQusWS7B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "\n",
        "        for i in range(1, target.shape[1]):\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = loss / int(target.shape[1])\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss\n"
      ],
      "metadata": {
        "id": "aFvFNl2aWaBD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(image_path):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image_path)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(100):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result.append(tokenizer.index_word.get(predicted_id, ''))\n",
        "\n",
        "        if tokenizer.index_word.get(predicted_id, '') == '<end>':\n",
        "            return result\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "HF1s5Z9PWwmQ"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}